{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import data and add default data to main dataframe\n",
    "df = pd.read_csv('/Users/chrisjackson/XXXX/1_Financial Data.csv')\n",
    "df2 = pd.read_csv('/Users/chrisjackson/XXXX/2_Default Data.csv')\n",
    "df['default'] = np.where(df['LOAN_ID'].isin(df2['LOAN_ID']), 1, 0)\n",
    "\n",
    "# set X and y\n",
    "X = df.drop(['default', 'LOAN_ID'], axis=1)\n",
    "y = df['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# set up pipeline for imputation and scaling of categorical variables and numerical variables\n",
    "num_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "cat_transform = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# get column indexes for categorical and numerical variables\n",
    "num_features = df.select_dtypes(include=['int64', 'float64']).drop([\n",
    "    'PD_RISK_RATING', 'default'], axis=1)\n",
    "cat_features = df.select_dtypes(include=['object']).drop(['LOAN_ID'], axis=1)\n",
    "\n",
    "numeric_cols = df.dtypes.apply(lambda x: x.kind in 'bifc').reset_index(\n",
    "    drop=True).loc[lambda x: x == True].index\n",
    "cat_cols = (df.dtypes == 'object').reset_index(\n",
    "    drop=True).loc[lambda x: x == True].index\n",
    "\n",
    "# set up column transformer for categorical and numerical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transform,  selector(dtype_exclude=\"object\")),\n",
    "        ('cat', cat_transform, selector(dtype_include=\"object\"))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   LOAN_ID         10000 non-null  object \n",
      " 1   PD_RISK_RATING  10000 non-null  int64  \n",
      " 2   X1              10000 non-null  object \n",
      " 3   X2              9000 non-null   float64\n",
      " 4   X3              10000 non-null  float64\n",
      " 5   X4              9000 non-null   float64\n",
      " 6   X5              10000 non-null  float64\n",
      " 7   X6              10000 non-null  float64\n",
      " 8   X7              10000 non-null  float64\n",
      " 9   X8              10000 non-null  float64\n",
      " 10  X9              10000 non-null  float64\n",
      " 11  X10             10000 non-null  float64\n",
      " 12  X11             10000 non-null  float64\n",
      " 13  X12             10000 non-null  float64\n",
      " 14  X13             10000 non-null  object \n",
      " 15  X14             10000 non-null  object \n",
      " 16  X15             10000 non-null  object \n",
      " 17  default         10000 non-null  int64  \n",
      "dtypes: float64(11), int64(2), object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pipe = preprocessor.fit_transform(X_train)\n",
    "X_test_pipe = preprocessor.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaze the hyperparameters for each model\n",
    "paramRF = {'n_estimators': [10, 50, 100, 250],\n",
    "           'max_depth': [5, 10, 20],\n",
    "           'class_weight': [None, {0: 1, 1: 5}, {0: 1, 1: 10}, {0: 1, 1: 25}]}\n",
    "\n",
    "paramSVC = {'C': [1, 10, 100],\n",
    "            'gamma': [1, 0.1, 0.001, 0.0001],\n",
    "            'kernel': ['linear', 'rbf']}\n",
    "\n",
    "paramLR = {'penalty': ['l1', 'l2'],\n",
    "           'C': np.logspace(-3, 3, 7),\n",
    "           'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "           }\n",
    "\n",
    "paramDT = {'max_depth': [5, 10, 25, None],\n",
    "           'max_features': ['sqrt', 'log2'],\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'class_weight': [None, {0: 1, 1: 5},\n",
    "                            {0: 1, 1: 10}, {0: 1, 1: 25}],\n",
    "           'criterion': ['gini', 'entropy']}\n",
    "\n",
    "paramKN = {'n_neighbors': [2, 5, 10, 25, 50]}\n",
    "\n",
    "paramHGB = {'learning_rate': (0.01, 0.1, 1, 10),\n",
    "            'max_leaf_nodes': (3, 10, 30)}\n",
    "\n",
    "paramXGB = {'min_child_weight': [1, 5, 10],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'max_depth': [3, 4, 5]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the various models for classification\n",
    "models = [\n",
    "    {'model': RandomForestClassifier(random_state=42), 'param': paramRF},\n",
    "    {'model': SVC(probability=True, random_state=42), 'param': paramSVC},\n",
    "    {'model': LogisticRegression(random_state=42), 'param': paramLR},\n",
    "    {'model': DecisionTreeClassifier(random_state=42), 'param': paramDT},\n",
    "    {'model': KNeighborsClassifier(), 'param': paramKN},\n",
    "    {'model': HistGradientBoostingClassifier(random_state=42), 'param': paramHGB},\n",
    "    {'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'), 'param': paramXGB}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to loop through models and hyperparameters and return results\n",
    "\n",
    "def run_models(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "      results = []\n",
    "      target_names = ['no default', 'default']\n",
    "      for model in models:\n",
    "            print(\" Results from Grid Search \", model['model'])\n",
    "            gridSearch = GridSearchCV(\n",
    "                  model['model'], model['param'], cv=3, scoring='f1_micro', verbose = 1, n_jobs=-1)\n",
    "            gridSearch.fit(X_train, y_train)\n",
    "            print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "                  gridSearch.best_estimator_)\n",
    "            print(\"\\n The best score across ALL searched params:\\n\",\n",
    "                  gridSearch.best_score_)\n",
    "            print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "                  gridSearch.best_params_)\n",
    "            predic = gridSearch.predict(X_test)\n",
    "            print(classification_report(y_test, predic))\n",
    "            results.append({'name': model['model'], \n",
    "                            'dataframe': pd.DataFrame(gridSearch.cv_results_),'best_estimator': gridSearch.best_estimator_,'report': classification_report(y_test, predic, target_names=target_names, output_dict=True)})\n",
    "      return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search  RandomForestClassifier(random_state=42)\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " RandomForestClassifier(class_weight={0: 1, 1: 25}, max_depth=5, random_state=42)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9927145302799408\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'class_weight': {0: 1, 1: 25}, 'max_depth': 5, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2961\n",
      "           1       0.67      0.67      0.67        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.83      0.83      0.83      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      " Results from Grid Search  SVC(probability=True, random_state=42)\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " SVC(C=1, gamma=1, kernel='linear', probability=True, random_state=42)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9925714690787629\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2961\n",
      "           1       0.92      0.59      0.72        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.96      0.79      0.86      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      " Results from Grid Search  LogisticRegression(random_state=42)\n",
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "42 fits failed out of a total of 126.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1094, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1094, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.98714286 0.98714286 0.98714286 0.98714286\n",
      "        nan        nan 0.98714286 0.98714286 0.98714286 0.98714286\n",
      "        nan        nan 0.98928584 0.99100012 0.99100012 0.98814294\n",
      "        nan        nan 0.99342849 0.99257141 0.99257141 0.99214284\n",
      "        nan        nan 0.9927141  0.99242847 0.99242847 0.99242847\n",
      "        nan        nan 0.99257122 0.99257122 0.99257122 0.99257122\n",
      "        nan        nan 0.99257122 0.99257122 0.99257122 0.99257122]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " LogisticRegression(penalty='l1', random_state=42, solver='liblinear')\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9934284895393919\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'C': 1.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2961\n",
      "           1       0.90      0.67      0.76        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.95      0.83      0.88      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      " Results from Grid Search  DecisionTreeClassifier(random_state=42)\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='log2',\n",
      "                       random_state=42)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.98885708118665\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'class_weight': None, 'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'min_samples_split': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2961\n",
      "           1       0.67      0.31      0.42        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.83      0.65      0.71      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      " Results from Grid Search  KNeighborsClassifier()\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " KNeighborsClassifier()\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9882857546181466\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'n_neighbors': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2961\n",
      "           1       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.49      0.50      0.50      3000\n",
      "weighted avg       0.97      0.99      0.98      3000\n",
      "\n",
      " Results from Grid Search  HistGradientBoostingClassifier(random_state=42)\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " HistGradientBoostingClassifier(max_leaf_nodes=10, random_state=42)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9935717956035585\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.1, 'max_leaf_nodes': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2961\n",
      "           1       1.00      0.59      0.74        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       1.00      0.79      0.87      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      " Results from Grid Search  XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, eval_metric='logloss', gamma=None,\n",
      "              gpu_id=None, importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None)\n",
      "Fitting 3 folds for each of 405 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chrisjackson/miniforge3/envs/mini_env/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.6,\n",
      "              enable_categorical=False, eval_metric='logloss', gamma=1,\n",
      "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=10, num_parallel_tree=1,\n",
      "              predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=1, subsample=0.6, tree_method='exact',\n",
      "              use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9944288160641873\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2961\n",
      "           1       0.92      0.59      0.72        39\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.96      0.79      0.86      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_models(X_train_pipe, y_train, X_test_pipe, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('NoSmoteResultsF1.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mini_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f75587dacee627cd4f922b679bcd65529678f042ef87d889d13fa7dbfc6775aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
